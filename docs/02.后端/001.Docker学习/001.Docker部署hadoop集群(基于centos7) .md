---
title: Docker部署hadoop集群(基于centos7)
date: 2023-11-12 17:04:02
permalink: /pages/e80c8e/
categories:
  - 后端
  - Docker学习
tags:
  - Docker
author: 
  name: 帅德华
  link: https://github.com/s-dehua
---
<!-- more -->

# 安装包准备

## docker

docker官网下载`https://www.docker.com/products/docker-desktop/`

## jdk

oracle官网下载`https://www.oracle.com/java/technologies/downloads/`

本文档下载的是jdk8u311（经过验证，hadopp2.7.6的版本不支持jdk9及以上，所以建议改为jdk8u311）

## hadoop

apache镜像站下载`http://archive.apache.org/dist/hadoop/common/`

本文档下载的是3.3.4的版本

# 启动centos容器

拉取centos7的镜像：

```shell
docker pull centos:7
```

启动容器：

```shell
docker run -itd --name hadoop01 -p 2201:22 -p 8088:8088 -p 9000:9000 -p 50070:50070 --privileged=true centos:7 /sbin/init
```

```shell
# 映射所有端口
docker run -itd --name devapp --net=host --privileged=true centos:7 /sbin/init
```



> 注意：
> -p:表示端口映射，这很重要，可以方便本机在外部访问web网页 需要设置容器和本机的相关端口映射
> -i:表示运行的容器
> -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。
> -d: 在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。
> –name :为创建的容器命名。
> –privileged：为true时赋予容器内root用户真正的root权限，否则root为普通用户,默认为flase
> /sbin/init: 使容器启动后可以使用systemctl方法

通过如下命令进入容器,containerID也可以填容器的名称：

```shell
docker exec -it containerID /bin/bash
```



# 安装Java

首先mac开启两个终端，一个是容器终端，另一个是本机终端

centos终端：创建dowload文件夹

```shell
mkdir /home/download
```

本机终端，移动到jdk存放目录，将jdk的包上传至dowload文件夹，hadoop安装包也相同上传

```shell
cd ~/software
docker cp jdk-8u381-linux-aarch64.tar.gz hadoop01:/home/download/
docker cp hadoop-3.3.4.tar.gz hadoop01:/home/download/
```

centos终端：进入download目录下将jdk文件解压至/usr/local/目录下

```shell
cd /home/download/
tar -zxvf jdk-8u381-linux-aarch64.tar.gz -C /usr/local/
```


进入jdk解压目录，将文件名修改为jdk(方便设置环境变量及简洁)：

```shell
cd /usr/local/
mv mv jdk1.8.0_311/ jdk
```

修改bashrc环境变量

```shell
vim /etc/bashrc
```

在末尾添加以下内容
```shell
#jdk environment
export JAVA_HOME=/usr/local/jdk
export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
#source 使环境变量生效
source /etc/bashrc
```



# 制作镜像并创建另两个容器

将hadoop01导出到镜像，并利用该镜像创建两个相同容器

```shell
#导出镜像
docker commit hadoop01 hadoopbase
#查看镜像列表
docker images
#创建相同容器
docker run -itd --name hadoop02 -p 2202:22 -p 50090:50090 --privileged=true hadoopbase /sbin/init
docker run -itd --name hadoop03 -p 2203:22 --privileged=true hadoopbase /sbin/init
```

- 注意：创建的容器进入之后root之后@的是容器id,在容器内使用

  ```shell
  hostname hadoop01
  ```

  可以对其主机名进行修改，ctrl+p+q之后即可生效

# 安装hadoop

## 统一网段

> 安装hadoop需要保持服务器之间内网连通，而我们创建的三个容器：hadoop01、hadoop02、hadoop03；默认是放在bridge的网段的,默认是联通的，但是为了和其他不相关的容器区分开，建议还是创建一个新的网段让三台容器自己相连。

```shell
#查看docker 存在的网段
docker network ls
#创建名为bigdata的新网段
docker network create bigdata
# 三台容器连入bigdata网段
docker network connect bigdata hadoop01
docker network connect bigdata hadoop02
docker network connect bigdata hadoop03
#断开三台容器与bridge的连接
docker network disconnect bridge hadoop01
docker network disconnect bridge hadoop02
docker network disconnect bridge hadoop03
#最后查看bigdata内的网段：以及三台机器的ip地址
docker network inspect bigdata
```

## SSH无密登录

对于hadoop01主机：

```shell
yum -y install passwd openssh-server openssh-clients
systemctl status sshd
systemctl start sshd
systemctl enable sshd #让sshd服务开机启动
ss -lnt #检查22端口号是否开通
```

设置root密码:

```
passwd root
```

> 备忘：
>
> root：123456

**注意，到这个步骤，每个容器都必须安装一次！**



安装完成之后：修改hosts文件

```shell
vim /etc/hosts
```

`#在文件后添加
172.18.0.4      hadoop01
172.18.0.3      hadoop02
172.18.0.2      hadoop03`

设置免密登录：

```shell
ssh-keygen -t rsa #连续三个回车
[root@hadoop01 ~] cd .ssh/
[root@hadoop01 ~] ls
```

将公钥拷贝到要免密登录的目标机器上：

```shell
ssh-copy-id hadoop01
ssh-copy-id hadoop02
ssh-copy-id hadoop03
```

**注意这里本容器的公钥也需要拷贝，以上命令需要在三个容器中都执行!**

## 容器间时间同步

为三台容器都下载chronyd：

```shell
yum -y install chrony
```

设置hadoop01为时间同步主服务器，其余节点从hadoop01同步时间：

```shell
#对于hadoop01容器
vim /etc/chrony.conf
```

取消该两行注释：

```shell
# Allow NTP client access from local network.
#allow 192.168.0.0/16

# Serve time even if not synchronized to a time source.
#local stratum 10
```

- 取消掉图中两行注释，前者代表允许该网段从本服务器同步时间，后者代表将本服务器作为时间同步主服务器
- 修改后启动chrony服务

```shell
 systemctl status chronyd #查看服务状态
 systemctl start chronyd #启动服务
 systemctl enable chronyd #将服务设置为开机启动
```

对于hadoop02和hadoop03 都修改时间同步来源为hadoop01：

```shell
vim /etc/chrony.conf 

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server hadoop01 iburst
```

一些chrony命令：

```text
查看时间同步源：
$ chronyc sources -v
查看时间同步源状态：
$ chronyc sourcestats -v
校准时间服务器：
$ chronyc tracking
```

## 解压hadoop包

解压文件：

```shell
 tar -zxvf hadoop-3.3.4.tar.gz -C /usr/local
```

配置hadoop环境变量：

```shell
cd /etc/profile.d/
touch my_env.sh
vim my_env.sh
#在其末尾添加
#HADOOP_HOME
export HADOOP_HOME=/usr/local/hadoop-3.3.4
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
#让修改后的文件生效
source /etc/profile
#测试是否成功
hadoop version
```

分发hadoop 以及环境变量:

```shell
cd /usr/local/
scp -r hadoop-3.3.4/ hadoop02:$PWD
scp -r hadoop-3.3.4/ hadoop03:$PWD
scp /etc/profile hadoop02:/etc/
scp /etc/profile hadoop03:/etc/

source /etc/profile #在hadoop02上执行
source /etc/profile #在hadoop03上执行
```

## 集群分发脚本

将脚本放在全局环境变量中

```shell
echo $PATH
```

在全局环境目录下创建xsync脚本：

```shell
cd /usr/local/bin
vim xsync

#编写如下脚本
#!/bin/bash

if [ $# -lt 1 ]
then
    echo Not Enough Arguement!
    exit;
fi
for host in hadoop01 hadoop02 hadoop03
do
    echo ====================  $host  ====================

    for file in $@
    do
        if [ -e $file ]
            then
                pdir=$(cd -P $(dirname $file); pwd)

                fname=$(basename $file)
                ssh $host "mkdir -p $pdir"
                rsync -av $pdir/$fname $host:$pdir
            else
                echo $file does not exists!
        fi
    done
done

#修改脚本xysnc 赋予执行权限
chmod +x xsync
#测试脚本
xsync /usr/local/bin
#将脚本复制/bin目录，以便全局调用
cp xsync /bin/
#同步环境变量配置
 ./bin/xsync /etc/profile.d/my_env.sh
 
#最后hadoop01 hadoop02 hadoop03 都执行 
source /etc/profile

```

8.关于hadoop集群的配置问题
可参考Hadoop的学习笔记（二）：完全分布式的配置、格式化、启动 - 知乎 (zhihu.com)
需要注意的点是：

> webUI界面：
> 在启动了hdfs和yarn之后，其HDFS的Yarn的webUI的界面地址：其不是容器的ip地址！！要注意
> 因为可以发现其实在物理机器上是ping不通hdfs的ip地址的
> ~~我也按照一些资料去解决问题，暂时还没有解决物理机和docker容器ip不互通的问题~~
> ~~但是，在启动容器的时候，我们设置了端口的映射关系，就可以通过物理机的ip地址来访问容器内端口因此：~~
>
> 网段桥接模式无ip分配！

HDFS:http://localhost:50070/
yarn:http://localhost:8080

# 配置hadoop

要实现Hadoop集群的配置，主要有三种方式：

1. 代码设置 >
2. *-site.xml 
3. *-default.xml

本章要实现完全分布式的配置，需要配置以下文件：

- hadoop-env.sh 
- yarn-env.sh 
- core-site.xml 
- hdfs-site.xml 
- mapred-site.xml 
- yarn-site.xml

在Hadoop安装完成后，会在$HADOOP_HOME/share路径下，有若干个*-default.xml文件，这些文件中记录了默认的配置信息。同时，在代码中，我们也可以设置Hadoop的配置信息。 这些位置配置的Hadoop，优先级为: 代码设置 > *-site.xml > *-default.xml

本次部署的集群规划如下：

```text
+--------------+---------------------+
|     Node     | Applications        |
+--------------+---------------------+
|  hadoop01    | NameNode            |
|              | DataNode            |
|              | ResourceManager     |
|              | NodeManager        |
+--------------+---------------------+
|  hadoop02    | SecondaryNameNode   |
|              | DataNode            |
|              | NodeManager         |
+--------------+---------------------+
|  hadoop03    | DataNode            |
|              | NodeManager         |
+--------------+---------------------+
```

## 配置内容

### core-site.xml

```xml
# cd $HADOOP_HOME/etc/hadoop/
# vi core-site.xml
<configuration>
    <!-- hdfs的地址名称：schame,ip,port-->
    <!-- 在Hadoop1.x的版本中，默认使用的端口是9000。在Hadoop2.x的版本中，默认使用端口是8020 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:8020</value>
    </property>
    <!-- hdfs的基础路径，被其他属性所依赖的一个基础路径 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/hadoop/tmp</value>
    </property>
</configuration>
```

属性解释：

*1.fs.defaultFS：*

HDFS对外提供服务的主机及端口号；

端口也可以省略不写。

*2.hadoop.tmp.dir：*

指定的HDFS文件存储的目录；

### hdfs-site.xml

```xml
# vi hdfs-site.xml
<configuration>
    <!-- namenode守护进程管理的元数据文件fsimage存储的位置-->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/name</value>
    </property>
    <!-- 确定DFS数据节点应该将其块存储在本地文件系统的何处-->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/data</value>
    </property>
    <!-- 块的副本数-->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 块的大小(128M),下面的单位是字节-->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <!-- secondarynamenode守护进程的http地址：主机名和端口号。参考守护进程布局-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop02:50090</value>
    </property>
  	<!-- namenode守护进程的http地址：主机名和端口号。参考守护进程布局-->
	<property>
  	  <name>dfs.namenode.http-address</name>
  	  <value>hadoop01:50070</value>
	</property>  
</configuration>
```

重要属性解释：

*1.dfs.namenode.name.dir：*

fsimage存储的位置

*2.dfs.datanode.data.dir：*

块的存储位置

*3.dfs.replication：*

HDFS为了保证属性不丢失，会保存块的副本

*4.dfs.blocksize：*

块大小，在hadoop1.x版本中为64M，在hadoop2.x的版本汇总默认128M

*5.dfs.namenode.secondary.http-address：*

指定secondarynamenode的节点服务器位置

*6.dfs.namenode.http-address：*

webui查看时的地址端口

### mapred-site.xml

```xml
# cp mapred-site.xml.template  mapred-site.xml
# vi mapred-site.xml
<configuration>
    <!-- 指定mapreduce使用yarn资源管理器-->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!-- 配置作业历史服务器的地址-->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop01:10020</value>
    </property>
    <!-- 配置作业历史服务器的http地址-->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop01:19888</value>
    </property>
</configuration>
```

### yarn-site.xml

```xml
# vi yarn-site.xml
<configuration>
    <!-- 指定yarn的shuffle技术-->
    <property>
        <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定resourcemanager的主机名-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop01</value>
    </property> 
    <!--下面的可选-->
    <!--指定shuffle对应的类 -->
	<property> 
	<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
     <value>org.apache.hadoop.mapred.ShuffleHandler</value> 
	</property>

	<!--配置resourcemanager的内部通讯地址-->
	<property>
	<name>yarn.resourcemanager.address</name>
	<value>hadoop01:8032</value>
	</property>

	<!--配置resourcemanager的scheduler的内部通讯地址-->
	<property>
	<name>yarn.resourcemanager.scheduler.address</name>
	<value>hadoop01:8030</value>
	</property>

	<!--配置resoucemanager的资源调度的内部通讯地址-->
	<property>
	<name>yarn.resourcemanager.resource-tracker.address</name>
	<value>hadoop01:8031</value>
	</property>

	<!--配置resourcemanager的管理员的内部通讯地址-->
	<property>
	<name>yarn.resourcemanager.admin.address</name>
	<value>hadoop01:8033</value>
	</property>

	<!--配置resourcemanager的web ui 的监控页面-->
	<property>
	<name>yarn.resourcemanager.webapp.address</name>
	<value>hadoop01:8088</value>
	</property>
</configuration>
```

### hadoop-env.sh

主要是配置hadoop运行的jdk环境路径

```bash
vim hadoop-env.sh
# The java implementation to use.
export JAVA_HOME=/usr/local/jdk
```

### yarn-env.sh

主要是配置yarn运行的jdk环境路径

```shell
vim yarn-site.sh
# some Java parameters
export JAVA_HOME=/usr/local/jdk
```

### slaves

slaves主要是配置集群的从节点

(注意：hadoop3.0之后更改workers文件而非slaves文件)

```text
vim slaves
hadoop01
hadoop02
hadoop03  
```

### 分发配置文件

```text
cd ..
scp -r hadoop/ hadoop02:$PWD
scp -r hadoop/ hadoop03:$PWD
```

## 格式化

### 在hadoop01上运行

PS:如果之前格式化过集群，需要删除上次配置的hadoop.tmp.dir属性对应位置的tmp文件夹，hadoop.tmp.dir属性值见core-site.xml文件

```text
hdfs namenode -format
```

### 格式化信息解读

```text
1. 生成一个集群唯一标识符:clusterid
2. 生成一个块池唯一标识符:blockPoolId
3. 生成namenode进程管理内容(fsimage)的存储路径：
	默认配置文件属性hadoop.tmp.dir指定的路径下生成dfs/name目录
4. 生成镜像文件fsimage，记录分布式文件系统根路径的元数据
5. 其他信息都可以查看一下，比如块的副本数，集群的fsOwner等。
```

### 目录所在位置及内容查看

![img](https://s-dehua.github.io/assets/v2-40290cb7ce0114789250539997422d7d_1440w.webp)

## 启动集群

### 启动脚本和关闭脚本介绍

```bash
1. 启动脚本
	-- start-dfs.sh			:用于启动hdfs集群的脚本
	-- start-yarn.sh		:用于启动yarn守护进程
	-- start-all.sh			:用于启动hdfs和yarn
2. 关闭脚本
	-- stop-dfs.sh			:用于关闭hdfs集群的脚本
	-- stop-yarn.sh			:用于关闭yarn守护进程
	-- stop-all.sh			:用于关闭hdfs和yarn
3. 单个守护进程脚本
	-- hadoop-daemons.sh	:用于单独启动或关闭hdfs的某一个守护进程的脚本
	-- hadoop-daemon.sh		:用于单独启动或关闭hdfs的某一个守护进程的脚本
	reg:
		hadoop-daemon.sh [start|stop] [namenode|datanode|secondarynamenode]
	
	-- yarn-daemons.sh	:用于单独启动或关闭hdfs的某一个守护进程的脚本
	-- yarn-daemon.sh		:用于单独启动或关闭hdfs的某一个守护进程的脚本
	reg:
		yarn-daemon.sh [start|stop] [resourcemanager|nodemanager]
```

### 启动HDFS

使用start-dfs.sh，启动hdfs

```text
- 启动集群中的各个机器节点上的分布式文件系统的守护进程
  一个namenode和resourcemanager以及secondarynamenode
  多个datanode和nodemanager
- 在namenode守护进程管理内容的目录下生成edit日志文件
- 在每个datanode所在节点下生成${hadoop.tmp.dir}/dfs/data目录,参考下图：
```

注意！ 如果哪台机器的相关守护进程没有开启，那么，就查看哪台机器上的守护进程对应的日志log文件,注意，启动脚本运行时提醒的日志后缀是*.out，而我们查看的是*.log文件。此文件的位置：${HADOOP_HOME}/logs/里

jps查看进程

```text
--1. 在hadoop01上运行jps指令，会有如下进程
	namenode
	datanode
--2. 在hadoop02上运行jps指令，会有如下进程
	secondarynamenode
	datanode
--3. 在hadoop03上运行jps指令，会有如下进程
	datanode 
```

### 启动yarn

使用start-yarn.sh脚本

jps查看进程

```text
--1. 在hadoop01上运行jps指令，会多出有如下进程
	resoucemanager
	nodemanager
--2. 在hadoop02上运行jps指令，会多出有如下进程
	nodemanager
--3. 在hadoop03上运行jps指令，会多出有如下进程
	nodemanager 
```

### web UI查看

```text
HDFS: http://10.211.55.3:50070
YARN: http://10.211.55.3:8088
```

网页截图如下：

hdfs：

![img](https://s-dehua.github.io/assets/v2-3e71979eef0fdb7e244d3a0e5559c208_1440w.webp)

yarn:

![img](https://s-dehua.github.io/assets/v2-714fb59d20dc25181a8b3b0be52845ff_1440w.webp)

如果网站能够正常打开，并如上图所示，代表已经部署并启动成功了！



